{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e1b22563b121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtest_tube\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from test_tube import Experiment\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : try to fix masking (true mask and virtual node)\n",
    "# Naming of variables (ie original nodes embed vs nodes embed), keep the same terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Coordinates autoencoder model\n",
    "class CoordAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_max, dim_node, dim_edge, hidden_node_dim, dim_f, \\\n",
    "                batch_size, \\\n",
    "                mpnn_steps=5, alignment_type='default', tol=1e-5, \\\n",
    "                use_X=True, use_R=True, virtual_node=False, seed=0, \\\n",
    "                refine_steps=0, refine_mom=0.99):\n",
    "        \n",
    "        super(CoordAE, self).__init__()\n",
    "        \n",
    "        # set random seed\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.mpnn_steps = mpnn_steps\n",
    "        self.n_max = n_max\n",
    "        self.dim_node = dim_node\n",
    "        self.dim_edge = dim_edge\n",
    "        self.hidden_node_dim = hidden_node_dim\n",
    "        self.dim_f = dim_f\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.virtual_node = virtual_node\n",
    "        self.refine_steps = refine_steps\n",
    "        self.refine_mom = refine_mom\n",
    "        self.use_X = use_X\n",
    "        self.use_R = use_R\n",
    "            \n",
    "        #self.G not included\n",
    "        #placeholders not included\n",
    "        #prior_T not included\n",
    "        \n",
    "        # find a way to define self.mask\n",
    "        \n",
    "        # ADD SELF IN ARGS FOR FOLLOWING LINES\n",
    "        self.embed_nodes = EmbedNode(batch_size, n_max, node_dim, hidden_dim)\n",
    "        \n",
    "        # Prior Z\n",
    "        self.edge_nn_prior_z = EdgeNN(batch_size, n_max, edge_dim + 1, hidden_dim)\n",
    "        self.mpnn_prior_z = MPNN(batch_size, n_max, hidden_dim, message_size, mpnn_steps)\n",
    "        self.latent_nn_prior_z = LatentNN(batch_size, n_max, hidden_dim, dim_f, 2*hidden_node_dim)\n",
    "        \n",
    "        # Post Z\n",
    "        if use_R :\n",
    "            self.edge_nn_post_z = EdgeNN(batch_size, n_max, edge_dim + 2, hidden_dim)\n",
    "        else :\n",
    "            self.edge_nn_post_z = EdgeNN(batch_size, n_max, edge_dim + 1, hidden_dim)\n",
    "            \n",
    "        if use_X:\n",
    "            self.embed_nodes_pos = EmbedNode(batch_size, n_max, node_dim + 3, hidden_dim)\n",
    "            \n",
    "        self.mpnn_post_z = MPNN(batch_size, n_max, hidden_dim, message_size, mpnn_steps)\n",
    "        self.latent_nn_post_z = LatentNN(batch_size, n_max, hidden_dim, dim_f, 2*hidden_node_dim)\n",
    "        \n",
    "        # Post X\n",
    "        self.edge_nn_post_x = EdgeNN(batch_size, n_max, edge_dim + 1, hidden_dim)\n",
    "        self.mpnn_post_x = MPNN(batch_size, n_max, hidden_dim, message_size, mpnn_steps)\n",
    "        self.latent_nn_post_x = LatentNN(batch_size, n_max, hidden_dim, dim_f, 3)\n",
    "        \n",
    "        # Post X det\n",
    "        self.edge_nn_post_x_det = EdgeNN(batch_size, n_max, edge_dim + 1, hidden_dim)\n",
    "        self.mpnn_post_x_det = MPNN(batch_size, n_max, hidden_dim, message_size, mpnn_steps)\n",
    "        self.latent_nn_post_x_det = LatentNN(batch_size, n_max, hidden_dim, dim_f, 3)\n",
    "        \n",
    "        # Pred X\n",
    "        self.edge_nn_pred_x = EdgeNN(batch_size, n_max, edge_dim + 1, hidden_dim)\n",
    "        self.mpnn_pred_x = MPNN(batch_size, n_max, hidden_dim, message_size, mpnn_steps)\n",
    "        self.latent_nn_pred_x = LatentNN(batch_size, n_max, hidden_dim, dim_f, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def forward(self, nodes, edges, mask, pos, proximity) :\n",
    "            \n",
    "            \"\"\" Args :\n",
    "                    nodes : Tensor(batch_size, n_max, node_feature_size)\n",
    "                    edges : Tensor(batch_size, n_max, n_max, edge_feature_size)\n",
    "                    mask : Tensor(batch_size, n_max, 1)\n",
    "                    pos : Tensor(batch_size, n_max, 3)\n",
    "                    proximity : Tensor(batch_size, n_max, n_max) is actually a distance matrix\n",
    "                Returns :\n",
    "                    \n",
    "            \"\"\"\n",
    "            \n",
    "            # TBD\n",
    "            if self.virtual_node:\n",
    "                mask = self.true_masks\n",
    "            else:\n",
    "                mask = self.mask\n",
    "            \n",
    "            nodes_embed = self.embed_nodes(node, mask) # (batch_size, n_max, hidden_node_dim)\n",
    "            \n",
    "            n_atom = mask.permute(0, 2, 1).sum(2) # (batch_size, 1)\n",
    "            \n",
    "            tiled_n_atom = n_atom.view(self.batch_size, 1, 1, 1).repeat(1, self.n_max, self.n_max, 1) # (batch_size, n_max, n_max, 1)\n",
    "            \n",
    "            # Isn't there a better way to add n_atom in edge features ?\n",
    "            edge_2 = torch.cat([edges, tiled_n_atom], 3) # (batch_size, n_max, nmax, edge_feature_size + 1)\n",
    "            \n",
    "            \n",
    "            # p(Z|G) -- prior of Z\n",
    "            \n",
    "            priorZ_edge_wgt = self.edge_nn_prior_z(edge_2) #[batch_size, n_max, n_max, hidden_node_dim, hidden_node_dim]\n",
    "            priorZ_hidden = self.mpnn_prior_z(priorZ_edge_wgt, nodes_embed, mask) # (batch_size, n_max, hidden_node_dim), nodes_embed like\n",
    "            priorZ_out = self.latent_nn_prior_z(priorZ_hidden, nodes_embed, mask) # (batch_size, n_max, 2*hidden_dim)\n",
    "            \n",
    "            priorZ_mu, priorZ_lsgms = priorZ_out.split([self.hidden_dim, self.hidden_dim], 2)\n",
    "            priorZ_sample = self._draw_sample(priorZ_mu, priorZ_lsgms, mask)\n",
    "            \n",
    "            \n",
    "            # q(Z|R(X),G) -- posterior of Z, used R instead of X as input for simplicity, should be updated\n",
    "            \n",
    "            if use_R:\n",
    "                proximity_view = proximity.view(self.batch_size, self.n_max, self.n_max, 1)\n",
    "                edge_cat = torch.cat([edge_2, proximity_view], 3) #[batch_size, n_max, n_max, edge_feature_size + 2]\n",
    "                postZ_edge_wgt = self.edge_nn_post_z(edge_cat) #[batch_size, n_max, n_max, hidden_node_dim, hidden_node_dim]\n",
    "            else:\n",
    "                postZ_edge_wgt = self.edge_nn_post_z(self.edge_2) \n",
    "\n",
    "            if use_X:\n",
    "                nodes_pos = torch.cat([nodes, pos], 2) # (batch_size, n_max, node_dim + 3)\n",
    "                nodes_pos_embed = self.embed_nodes_pos(nodes_pos, mask)\n",
    "                postZ_hidden = self.mpnn_post_z(postZ_edge_wgt, nodes_pos_embed, mask)\n",
    "            else:\n",
    "                postZ_hidden = self.mpnn_post_z(postZ_edge_wgt, nodes_embed, mask)\n",
    "            \n",
    "            postZ_out = self.latent_nn_prior_z(postZ_hidden, nodes_embed, mask)\n",
    "            \n",
    "            postZ_mu, postZ_lsgms = postZ_out.split([self.hidden_dim, self.hidden_dim], 2)\n",
    "            postZ_sample = self._draw_sample(postZ_mu, postZ_lsgms, mask)\n",
    "            \n",
    "            \n",
    "            # p(X|Z,G) -- posterior of X\n",
    "            \n",
    "            X_edge_wgt = self.edge_nn_post_x(edge_2) #[batch_size, n_max, n_max, dim_h, dim_h]\n",
    "            X_hidden = self.mpnn_post_x(X_edge_wgt, postZ_sample + nodes_embed, mask)\n",
    "            X_pred = self.latent_nn_post_x(X_hidden, nodes_embed, mask)\n",
    "            \n",
    "            \n",
    "            # p(X|Z,G) -- posterior of X without sampling from latent space\n",
    "            # used for iterative refinement of predictions ; det stands for deterministic\n",
    "            \n",
    "            X_edge_wgt_det = self.edge_nn_post_x_det(self.edge_2) #[batch_size, n_max, n_max, dim_h, dim_h]\n",
    "            X_hidden_det = self.mpnn_post_x_det(X_edge_wgt_det, postZ_mu + nodes_embed, mask)\n",
    "            X_pred_det = self.latent_nn_post_x_det(X_hidden_det, nodes_embed, mask)\n",
    "            \n",
    "            \n",
    "            # Prediction of X with p(Z|G) in the test phase\n",
    "            \n",
    "            PX_edge_wgt = self.edge_nn_pred_x(edge_2) #[batch_size, n_max, n_max, dim_h, dim_h]\n",
    "            PX_hidden = self.mpnn_pred_x(PX_edge_wgt, priorZ_sample + nodes_embed, mask)\n",
    "            PX_pred = self.latent_nn_pred_x(PX_hidden, nodes_embed, mask)\n",
    "            \n",
    "            return postZ_mu, postZ_lsgms, priorZ_mu, priorZ_lsgms, X_pred, PX_pred\n",
    "        \n",
    "        def _draw_sample(self, mu, lsgms, mask):\n",
    "\n",
    "            epsilon = torch.randn_like(lsgms)\n",
    "            \n",
    "            sample = torch.mul(torch.exp(0.5 * lsgms), epsilon)\n",
    "            sample = torch.add(mu, sample)\n",
    "            sample = torch.mul(sample, mask)\n",
    "\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There may be a way to merge EmbedNode and EdgeNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedNode(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_max, node_dim, hidden_dim):\n",
    "        \n",
    "        super(EmbedNode, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_max = n_max\n",
    "        self.node_dim = node_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.FC_hidden = nn.Linear(node_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, nodes, mask):\n",
    "\n",
    "        \"\"\"\n",
    "            Args :\n",
    "                nodes :  Tensor(batch_size, n_max, node_dim)\n",
    "            Returns :\n",
    "                nodes_embed : Tensor(batch_size, n_max, hidden_node_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        nodes_view = nodes.view(self.batch_size * self.n_max, nodes.shape[2])\n",
    "\n",
    "        emb1 = torch.sigmoid(self.FC_hidden(nodes_view))\n",
    "        emb2 = torch.tanh(self.FC_output(emb1))\n",
    "\n",
    "        nodes_embed = emb2.view(emb2, [self.batch_size, self.n_max, self.hidden_dim])\n",
    "        nodes_embed = torch.mul(nodes_embed, mask)\n",
    "\n",
    "        return nodes_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_max, edge_dim, hidden_dim):\n",
    "        \n",
    "        super(EdgeNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_max = n_max\n",
    "        self.edge_dim = edge_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.FC_hidden = nn.Linear(edge_dim, 2 * hidden_dim)\n",
    "        self.FC_output = nn.Linear(2 * hidden_dim, hidden_dim * hidden_dim)\n",
    "        \n",
    "    def forward(self, edges):\n",
    "\n",
    "        \"\"\"\n",
    "            Args :\n",
    "                edges :  Tensor(batch_size, n_max, node_dim)\n",
    "            Returns :\n",
    "                edges_embed : Tensor(batch_size, n_max, n_max, hidden_dim, hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        edges_view = edges.view(self.batch_size * self.n_max * self.n_max, nodes.shape[3])\n",
    "\n",
    "        emb1 = torch.sigmoid(self.FC_hidden(edges_view))\n",
    "        emb2 = torch.tanh(self.FC_output(emb1))\n",
    "\n",
    "        edges_embed = emb2.view(emb2, [self.batch_size, self.n_max, self.n_max, self.hidden_dim, self.hidden_dim])\n",
    "\n",
    "        return edges_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_max, hidden_dim, message_size, mpnn_steps):\n",
    "        \n",
    "        super(MPNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_max = n_max\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.message_size = message_size\n",
    "        self.mpnn_steps = mpnn_steps\n",
    "        \n",
    "        self.gru = torch.nn.GRUCell(input_size=message_size, hidden_size=hidden_dim)\n",
    "        \n",
    "    def forward(self, edge_wgt, nodes_embed, mask): \n",
    "\n",
    "        \"\"\"\n",
    "            Args :\n",
    "                edge_wgt : Tensor(batch_size, n_max, n_max, hidden_dim, hidden_dim)\n",
    "                nodes_embed : Tensor(batch_size, n_max, hidden_node_dim)\n",
    "                mask : Tensor(batch_size, n_max, 1)\n",
    "            Returns :\n",
    "                nodes_embed : Tensor(batch_size, n_max, hidden_node_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.mpnn_steps):\n",
    "        \n",
    "            messages = self.msg_nn(edge_wgt, nodes_embed) # (batch_size, n_max, hidden_node_dim)\n",
    "\n",
    "            if true_mask and i == self.mpnn_steps - 1:\n",
    "                nodes_embed = self.update_GRU(messages, nodes_embed, mask)\n",
    "            else:\n",
    "                nodes_embed = self.update_GRU(messages, nodes_embed)\n",
    "\n",
    "        return nodes_embed\n",
    "    \n",
    "    def update_GRU(self, messages, nodes, mask):\n",
    "\n",
    "        \"\"\"\n",
    "            Args :\n",
    "                messages : Tensor(batch_size, n_max, hidden_dim)\n",
    "                nodes : Tensor(batch_size, n_max, hidden_dim)\n",
    "                mask : Tensor(batch_size, n_max, 1)\n",
    "            Returns :\n",
    "                nodes_next : Tensor(batch_size, n_max, hidden_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        messages = messages.view(self.batch_size * self.n_max, 1, self.hidden_dim) \n",
    "        nodes = nodes.view(self.batch_size * self.n_max, self.hidden_dim)\n",
    "\n",
    "        nodes_next = self.gru(messages, nodes)\n",
    "\n",
    "        nodes_next = nodes_next.view(self.batch_size, self.n_max, self.hidden_dim)\n",
    "        nodes_next = torch.mul(nodes_next, mask) #TBD\n",
    "\n",
    "        return nodes_next\n",
    "    \n",
    "    def compute_messages(self, edge_wgt, nodes) :\n",
    "        \n",
    "        \"\"\"\n",
    "            Args :\n",
    "                edge_wgt : Tensor(batch_size, n_max, hidden_dim)\n",
    "                nodes : Tensor(batch_size, n_max, hidden_node_dim)\n",
    "            Returns :\n",
    "                messages : Tensor(batch_size, n_max, hidden_node_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = edge_wgt.view(self.batch_size * self.n_max, self.n_max * self.hidden_dim, self.hidden_dim)\n",
    "        nodes = nodes.view(self.batch_size * self.n_max, self.hidden_dim, 1)\n",
    "\n",
    "        messages = torch.matmul(weights, nodes)\n",
    "        messages = messages.view(self.batch_size, self.n_max, self.n_max, self.hidden_dim)\n",
    "        messages = messages.permute(0, 2, 3, 1)\n",
    "        messages = messages.mean(3) / self.n_max\n",
    "\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, n_max, hidden_dim, dim_f, outdim):\n",
    "        \n",
    "        super(LatentNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_max = n_max\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dim_f = dim_f\n",
    "        self.outdim = outdim\n",
    "        \n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.FC_hidden = nn.Linear(hidden_dim, dim_f)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "        #self.FC_hidden2 = nn.Linear(dim_f, dim_f)\n",
    "        self.FC_output = nn.Linear(dim_f, outdim)\n",
    "        \n",
    "    def forward(self, nodes_embed, original_nodes_embed, mask):\n",
    "\n",
    "        \"\"\"\n",
    "            Args :\n",
    "                nodes_embed :  Tensor(batch_size, n_max, hidden_dim)\n",
    "                original_nodes_embed :  Tensor(batch_size, n_max, hidden_dim)\n",
    "            Returns :\n",
    "                nodes_embed : Tensor(batch_size, n_max, outdim)\n",
    "        \"\"\"\n",
    "        \n",
    "        nodes_cat = torch.cat([nodes_embed, original_nodes_embed], 2)\n",
    "        nodes_cat = nodes_cat.view(self.batch_size * self.n_max, nodes_concat.shape[2])\n",
    "        \n",
    "        nodes_cat = self.dropout1(nodes_cat)\n",
    "        nodes_cat = torch.sigmoid(self.FC_hidden(nodes_cat))\n",
    "        nodes_cat = self.dropout2(nodes_cat)\n",
    "        # nodes_cat = torch.sigmoid(self.FC_hidden2(nodes_cat))\n",
    "        nodes_cat = self.FC_output(nodes_cat)\n",
    "        \n",
    "        nodes_cat = nodes_cat.view(self.batch_size, self.n_max, outdim)\n",
    "        nodes_cat = torch.mul(nodes_cat, mask)\n",
    "\n",
    "        return nodes_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add underscore to internal functions\n",
    "\n",
    "class MSDScorer(object) :\n",
    "    def __init__(self, alignment_type='linear', tol=1e-5):\n",
    "        self.alignment_type = alignment_type\n",
    "        self.tol = tol\n",
    "        \n",
    "        if alignment_type == 'linear':\n",
    "            self.msd_func = self.linear_transform_msd\n",
    "        elif alignment_type == 'kabsch':\n",
    "            self.msd_func = self.kabsch_msd\n",
    "        elif alignment_type == 'default':\n",
    "            self.msd_func = self.mol_msd\n",
    "        \n",
    "    def score(self, X_pred, pos, mask=None) :\n",
    "        return self.msd_func(X_pred, pos, mask)\n",
    "        \n",
    "    def kabsch_msd(self, frames, targets, masks):\n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            frame = frames[i]\n",
    "            target = targets[i]\n",
    "            mask = masks[i]\n",
    "            target_cent = target - self.torch_centroid_masked(target, mask)\n",
    "            frame_cent = frame - self.torch_centroid_masked(frame, mask)\n",
    "            losses.append(self.torch_kabsch_rmsd_masked(target_cent.detach(), frame_cent, mask))\n",
    "\n",
    "        loss = torch.stack(losses)\n",
    "        return loss\n",
    "\n",
    "    def optimal_rotational_quaternion(self, r):\n",
    "        \"\"\"Just need the largest eigenvalue of this to minimize RMSD over rotations\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] http://dx.doi.org/10.1002/jcc.20110\n",
    "        \"\"\"\n",
    "        return [\n",
    "            [r[0][0] + r[1][1] + r[2][2], r[1][2] - r[2][1], r[2][0] - r[0][2], r[0][1] - r[1][0]],\n",
    "            [r[1][2] - r[2][1], r[0][0] - r[1][1] - r[2][2], r[0][1] + r[1][0], r[0][2] + r[2][0]],\n",
    "            [r[2][0] - r[0][2], r[0][1] + r[1][0], -r[0][0] + r[1][1] - r[2][2], r[1][2] + r[2][1]],\n",
    "            [r[0][1] - r[1][0], r[0][2] + r[2][0], r[1][2] + r[2][1], -r[0][0] - r[1][1] + r[2][2]],\n",
    "        ]\n",
    "    \n",
    "    def squared_deviation(self, frame, target):\n",
    "        \"\"\"Calculate squared deviation (n_atoms * RMSD^2) from `frame` to `target`\n",
    "        First we compute `R` which is the ordinary cross-correlation of xyz coordinates.\n",
    "        Turns out you can do a bunch of quaternion math to find an eigen-expression for finding optimal\n",
    "        rotations. There aren't quaternions in tensorflow, so we use the handy formula for turning\n",
    "        quaternions back into 4-matrices. This is the `F` matrix. We find its leading eigenvalue\n",
    "        to get the MSD after optimal rotation. Note: *finding* the optimal rotation requires the values\n",
    "        and vectors, but we don't care.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame, target : Tensor, shape=(n_atoms, 3)\n",
    "            Calculate the MSD between these two frames\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sd : Tensor, shape=(0,)\n",
    "            Divide by number of atoms and take the square root for RMSD\n",
    "        \"\"\"\n",
    "        R = torch.matmul(frame.T, target)\n",
    "        R_parts = [torch.unbind(t) for t in torch.unbind(R)]\n",
    "        F_parts = self.optimal_rotational_quaternion(R_parts)\n",
    "        F = torch.Tensor(F_parts)\n",
    "        vals, vecs = torch.symeig(F, eigenvectors=True)\n",
    "        # This isn't differentiable for some godforsaken reason.\n",
    "        # vals = tf.self_adjoint_eigvals(F, name='vals')\n",
    "        lmax = torch.unbind(vals)[-1]\n",
    "        sd = torch.sum(frame ** 2 + target ** 2) - 2 * lmax\n",
    "        return sd\n",
    "    \n",
    "    # https://towardsdatascience.com/tensorflow-rmsd-using-tensorflow-for-things-it-was-not-designed-to-do-ada4c9aa0ea2\n",
    "    # https://github.com/mdtraj/tftraj\n",
    "    def mol_msd(self, frames, targets, masks):\n",
    "        frames -= frames.mean(1, keepdim=True)\n",
    "        targets -= targets.mean(1, keepdim=True)\n",
    "\n",
    "        loss = torch.stack([self.squared_deviation( self.do_mask(frames[i], masks[i]), self.do_mask(targets[i], masks[i]) ) for i in range(batch_size)], 0)\n",
    "        return loss / masks.sum((1,2))\n",
    "\n",
    "    def linear_transform_msd(self, frames, targets, masks):\n",
    "        def linearly_transform_frames(padded_frames, padded_targets):\n",
    "            u, s, v = torch.svd(padded_frames)\n",
    "            tol = 1e-7\n",
    "            atol = s.max() * tol\n",
    "            s = torch.masked_select(s, s > atol)\n",
    "            s_inv = torch.diag(1. / s)\n",
    "            pseudo_inverse = torch.matmul(v, torch.matmul(s_inv, u.T))\n",
    "\n",
    "            weight_matrix = torch.matmul(padded_targets, pseudo_inverse)\n",
    "            transformed_frames = torch.matmul(weight_matrix, padded_frames)\n",
    "            return transformed_frames\n",
    "\n",
    "        padded_frames = torch.nn.functional.pad(frames, (0, 1), 'constant', 1)\n",
    "        padded_targets = torch.nn.functional.pad(targets, (0, 1), 'constant', 1)\n",
    "\n",
    "        mask_matrices = []\n",
    "        for i in range(batch_size):\n",
    "            mask_matrix = torch.diag(masks[i].view(-1))\n",
    "            mask_matrices.append(mask_matrix)\n",
    "        #mask_matrix = tf.diag(tf.reshape(masks, [self.batch_size, -1]))\n",
    "        mask_tensor = torch.stack(mask_matrices)\n",
    "        masked_frames = torch.matmul(mask_tensor, padded_frames)\n",
    "        masked_targets = torch.matmul(mask_tensor, padded_targets)\n",
    "        transformed_frames = []\n",
    "        for i in range(batch_size):\n",
    "            transformed_frames.append(linearly_transform_frames(masked_frames[i], masked_targets[i]))\n",
    "        transformed_frames = torch.stack(transformed_frames)\n",
    "        #transformed_frames = linearly_transform_frames(masked_frames, masked_targets)\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        loss = mse_loss(transformed_frames, masked_targets)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def torch_kabsch(self, P, Q):\n",
    "        # calculate covariance matrix\n",
    "        C = torch.matmul(P.T, Q)\n",
    "\n",
    "        V, S, W = torch.svd(C, some=False)\n",
    "        \n",
    "        def adjoint(W) :\n",
    "            W = torch.transpose(W, -2, -1)\n",
    "            W = torch.conj(W)\n",
    "            return W\n",
    "        \n",
    "        W = adjoint(W)\n",
    "\n",
    "        # implement the following numpy ops in pytorch ; could be factorized\n",
    "        # S[-1] = -S[-1]\n",
    "        # V[:, -1] = -V[:, -1]\n",
    "            \n",
    "        m1 = torch.ones((3,), dtype=torch.float32)\n",
    "        m1[-1] = -m1[-1]\n",
    "\n",
    "        m2 = torch.ones((3,3), dtype=torch.float32)\n",
    "        m2[:,-1] = -m2[:,-1]\n",
    "\n",
    "        d = torch.det(V) * torch.det(W)\n",
    "        S = torch.where(d < 0., S * m1, S)\n",
    "        V = torch.where(d < 0., V * m2, V)\n",
    "        # Rotation matrix U\n",
    "        U = torch.matmul(V, W)\n",
    "        return U\n",
    "\n",
    "    # maybe I could implement a batch RMSE implementation\n",
    "    # N could be handled by taking the number of rows different from [0 0 0] or [nan nan nan]\n",
    "    # depending on how padding is handled\n",
    "    \n",
    "    def torch_rmsd_masked(self, V, W, N=None):\n",
    "        \"\"\"\n",
    "        Compute the RMSD between the two coordinates matrices V and W.\n",
    "        N (int) is the number of atoms having coordinates (selected via masking in the workflow)\n",
    "        Args :\n",
    "            V : Tensor(n_max, 3)\n",
    "            W : Tensor(n_max, 3)\n",
    "            N : int\n",
    "        Returns :\n",
    "            RMSD : float\n",
    "        \"\"\"\n",
    "        if N is None :\n",
    "            N = V.shape[0]\n",
    "        SE = (V - W) ** 2 # SE = Squared Error\n",
    "        MSE = SE.sum() / N.float() # MSE = Mean Squared Error\n",
    "        return torch.sqrt(MSE) # RMSE = Mean Squared Error\n",
    "\n",
    "    def torch_kabsch_rotate(self, P, Q):\n",
    "        U = self.torch_kabsch(P, Q) # rotate matrix P\n",
    "        return torch.matmul(P, U)\n",
    "\n",
    "    def torch_kabsch_rmsd_masked(self, P, Q, mask=None):\n",
    "        N = None\n",
    "        if mask != None :\n",
    "            N = mask.sum()\n",
    "            mask_mat = torch.diag(mask.view((-1,)))\n",
    "            P = torch.matmul(mask_mat, P) + self.tol\n",
    "            Q = torch.matmul(mask_mat, Q) + self.tol\n",
    "        P_transformed = self.torch_kabsch_rotate(P, Q)\n",
    "        return self.torch_rmsd_masked(P_transformed, Q, N)\n",
    "\n",
    "    def torch_centroid_masked(self, P, mask=None):\n",
    "        N = P.shape[0]\n",
    "        if mask != None : # mask P\n",
    "            N = mask.sum()\n",
    "            mask_mat = torch.diag(mask.view((-1,)))\n",
    "            \n",
    "            P = torch.matmul(mask_mat, P) + self.tol\n",
    "        return P.sum(0, keepdim=True) / N.float()\n",
    "\n",
    "    def do_mask(self, vec, mask):\n",
    "        return vec[torch.gt(mask, 0.5).view((mask.shape[0],))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLD(mu0, lsgm0, mu1, lsgm1, mask):\n",
    "    \"\"\"\n",
    "    lsgm : log variance\n",
    "    Args :\n",
    "        mu0 : Tensor(batch_size, n_max, dim_h)\n",
    "        lsgm0 : Tensor(batch_size, n_max, dim_h)\n",
    "        mu1 : Tensor(batch_size, n_max, dim_h)\n",
    "        lsgm1 : Tensor(batch_size, n_max, dim_h)\n",
    "        mask : Tensor(batch_size, n_max, 1)\n",
    "        \n",
    "    Returns :\n",
    "        kld : Tensor(batch_size, n_max, dim_h)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    var0 = torch.exp(lsgm0)\n",
    "    var1 = torch.exp(lsgm1)\n",
    "    a = torch.div(var0 + 1e-5, var1 + 1e-5)\n",
    "    b = torch.div(torch.square(torch.subtract(mu1, mu0)), var1 + 1e-5)\n",
    "    c = torch.log(torch.div(var1 + 1e-5, var0 + 1e-5) + 1e-5)\n",
    "\n",
    "    kld = 0.5 * torch.sum(a + b - 1 + c, 2, keepdim=True) * mask\n",
    "\n",
    "    return kld\n",
    "\n",
    "\n",
    "def KLD_zero(mu0, lsgm0, mask):\n",
    "    \"\"\"\n",
    "    lsgm : log variance\n",
    "    Args :\n",
    "        mu0 : Tensor(batch_size, n_max, dim_h)\n",
    "        lsgm0 : Tensor(batch_size, n_max, dim_h)\n",
    "        mask : Tensor(batch_size, n_max, 1)\n",
    "    \n",
    "    Returns :\n",
    "        kld : Tensor(batch_size, n_max, dim_h)\n",
    "    \"\"\"\n",
    "    \n",
    "    a = torch.exp(lsgm0) + torch.square(mu0)\n",
    "    b = 1 + lsgm0\n",
    "\n",
    "    kld = 0.5 * torch.sum(a - b, 2, keepdim=True) * mask\n",
    "\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=True,  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training step\n",
    "# add model.train() optimizer zerograd ...\n",
    "\n",
    "D1_t, D2_t, D3_t, D4_t, D5_t, MS_t, D1_v, D2_v, D3_v, D4_v, D5_v, MS_v # given by train script\n",
    "\n",
    "batch_size = 20\n",
    "save_path = None\n",
    "train_event_path = None\n",
    "valid_event_path = None\n",
    "log_train_steps=100\n",
    "tm_trn=None\n",
    "tm_val=None\n",
    "w_reg=1e-3\n",
    "debug=False\n",
    "exp=None # Experiment\n",
    "\n",
    "model = CoordAE(n_max=50, dim_node=7, dim_edge=4, hidden_node_dim=15, dim_f=50,\n",
    "                batch_size=24)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "msd_scorer = MSDScorer('default')\n",
    "\n",
    "if exp is not None:\n",
    "    data_path = exp.get_data_path(exp.name, exp.version)\n",
    "    save_path = os.path.join(data_path, 'checkpoints/model.ckpt')\n",
    "    event_path = os.path.join(data_path, 'event/')\n",
    "    print(save_path, flush=True)\n",
    "    print(event_path, flush=True)\n",
    "    \n",
    "if not debug:\n",
    "    train_summary_writer = SummaryWriter(train_event_path)\n",
    "    valid_summary_writer = SummaryWriter(valid_event_path)\n",
    "\n",
    "# session\n",
    "n_batch_val = int(len(D1_v)/batch_size)\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "# training\n",
    "print('::: start training')\n",
    "num_epochs = 2500\n",
    "valaggr_mean = np.zeros(num_epochs)\n",
    "valaggr_std = np.zeros(num_epochs)\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_loader = TODO\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    [D1_t, D2_t, D3_t, D4_t, D5_t] = self._permutation([D1_t, D2_t, D3_t, D4_t, D5_t])\n",
    "\n",
    "    trnscores = np.zeros((len(train_loader), 4))\n",
    "    \n",
    "    # CHANGE AND USE TRAIN LOADER\n",
    "    for batch_idx, batch in enumerate(train_loader) :\n",
    "        \n",
    "        # batch to be created\n",
    "        nodes, masks, edges, proximity, pos = batch\n",
    "        \n",
    "        postZ_mu, postZ_lsgms, priorZ_mu, priorZ_lsgms, X_pred, PX_pred = model(nodes, edges, masks, pos, proximity)\n",
    "    \n",
    "        cost_KLDZ = torch.mean(torch.sum(KLD(postZ_mu, postZ_lsgms, priorZ_mu, priorZ_lsgms, mask), (1, 2))) # posterior | prior\n",
    "        cost_KLD0 = torch.mean(torch.sum(KLD_zero(priorZ_mu, priorZ_lsgms, mask), (1, 2))) # prior | N(0,1)\n",
    "\n",
    "        #mask = self.true_masks if self.virtual_node else self.mask\n",
    "        cost_X = torch.mean(msd_scorer.score(X_pred, pos, mask))\n",
    "\n",
    "        cost_op = cost_X + cost_KLDZ + w_reg * cost_KLD0\n",
    "\n",
    "        if debug:\n",
    "            print (i, n_batch)\n",
    "            print(trnresult, flush=True)\n",
    "\n",
    "        # log results\n",
    "        curr_iter = epoch * len(train_loader) + batch_idx # maybe it's len dataloader\n",
    "\n",
    "        if not debug:\n",
    "            if curr_iter % log_train_steps == 0:\n",
    "                train_summary_writer.add_scalar(\"train/cost_op\", trnresult[0], curr_iter)\n",
    "                train_summary_writer.add_scalar(\"train/cost_X\", trnresult[1], curr_iter)\n",
    "                train_summary_writer.add_scalar(\"train/cost_KLDZ\", trnresult[2], curr_iter)\n",
    "                train_summary_writer.add_scalar(\"train/cost_KLD0\", trnresult[3], curr_iter)\n",
    "\n",
    "        assert np.sum(np.isnan(trnresult)) == 0\n",
    "        trnscores[i,:] = trnresult\n",
    "        \n",
    "    print(np.mean(trnscores,0), flush=True)\n",
    "    \n",
    "    exp_dict = {}\n",
    "    if exp is not None:\n",
    "        exp_dict['training epoch id'] = epoch\n",
    "        exp_dict['train_score'] = np.mean(trnscores,0)\n",
    "\n",
    "    valscores_mean, valscores_std = self.test(D1_v, D2_v, D3_v, D4_v, D5_v, MS_v, \\\n",
    "                                    tm_v=tm_val, debug=debug)\n",
    "\n",
    "    valaggr_mean[epoch] = valscores_mean\n",
    "    valaggr_std[epoch] = valscores_std\n",
    "\n",
    "    if not debug:\n",
    "        valid_summary_writer.add_scalar(\"val/valscores_mean\", valscores_mean, epoch)\n",
    "        valid_summary_writer.add_scalar(\"val/min_valscores_mean\", np.min(valaggr_mean[0:epoch+1]), epoch)\n",
    "        valid_summary_writer.add_scalar(\"val/valscores_std\", valscores_std, epoch)\n",
    "        valid_summary_writer.add_scalar(\"val/min_valscores_std\", np.min(valaggr_std[0:epoch+1]), epoch)\n",
    "\n",
    "    print ('::: training epoch id {} :: --- val mean={} , std={} ; --- best val mean={} , std={} '.format(\\\n",
    "            epoch, valscores_mean, valscores_std, np.min(valaggr_mean[0:epoch+1]), np.min(valaggr_std[0:epoch+1])))\n",
    "    \n",
    "    if exp is not None:\n",
    "        exp_dict['val mean'] = valscores_mean\n",
    "        exp_dict['std'] = valscores_std\n",
    "        exp_dict['best val mean'] = np.min(valaggr_mean[0:epoch+1])\n",
    "        exp_dict['std of best val mean'] = np.min(valaggr_std[0:epoch+1])\n",
    "        exp.log(exp_dict)\n",
    "        exp.save()\n",
    "\n",
    "    # keep track of the best model as well in the separate checkpoint\n",
    "    # it is done by copying the checkpoint\n",
    "    if valaggr_mean[epoch] == np.min(valaggr_mean[0:epoch+1]) and not debug:\n",
    "        for ckpt_f in glob.glob(save_path + '*'):\n",
    "            model_name_split = ckpt_f.split('/')\n",
    "            model_path = '/'.join(model_name_split[:-1])\n",
    "            model_name = model_name_split[-1]\n",
    "            best_model_name = model_name.split('.')[0] + '_best.' + '.'.join(model_name.split('.')[1:])\n",
    "            full_best_model_path = os.path.join(model_path, best_model_name)\n",
    "            full_model_path = ckpt_f\n",
    "            shutil.copyfile(full_model_path, full_best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_to_proximity(pos, mask):\n",
    "    \"\"\" Args\n",
    "            pos : Tensor(batch_size, n_max, 3)\n",
    "            mask : Tensor(batch_size, n_max, 1)\n",
    "            \n",
    "        Returns :\n",
    "            proximity : Tensor(batch_size, n_max, nmax)\n",
    "    \"\"\"\n",
    "\n",
    "    pos_1 = pos.unsqueeze(2)\n",
    "    pos_2 = pos.unsqueeze(1)\n",
    "\n",
    "    pos_sub = torch.sub(pos_1, pos_2) #[batch_size, n_max, nmax, 3]\n",
    "    proximity = torch.square(pos_sub)\n",
    "    proximity = torch.sum(proximity, 3) #[batch_size, n_max, nmax]\n",
    "    proximity = torch.sqrt(proximity + 1e-5)\n",
    "\n",
    "    #proximity_view = torch.view(self.batch_size, self.n_max, self.n_max) I don't understand the rationale\n",
    "    proximity = torch.mul(proximity, mask)\n",
    "    proximity = torch.mul(proximity, mask.permute(0, 2, 1))\n",
    "\n",
    "    # set diagonal of distance matrix to 0\n",
    "    proximity[:, torch.arange(proximity.shape[1]), torch.arange(proximity.shape[2])] = 0\n",
    "\n",
    "    return proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSD(reference_mol, positions, useFF=False):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        reference_mol : RDKit.Molecule\n",
    "        positions : Tensor(n_atom, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def optimizeWithFF(mol):\n",
    "\n",
    "        mol = Chem.AddHs(mol, addCoords=True)\n",
    "        AllChem.MMFFOptimizeMolecule(mol)\n",
    "        mol = Chem.RemoveHs(mol)\n",
    "\n",
    "        return mol\n",
    "\n",
    "    n_atom = reference_mol.GetNumAtoms()\n",
    "\n",
    "    test_cf = Chem.rdchem.Conformer(n_atom)\n",
    "    for k in range(n_atom):\n",
    "        test_cf.SetAtomPosition(k, positions[k].tolist())\n",
    "\n",
    "    test_mol = copy.deepcopy(reference_mol)\n",
    "    test_mol.RemoveConformer(0)\n",
    "    test_mol.AddConformer(test_cf)\n",
    "\n",
    "    if useFF:\n",
    "        try:\n",
    "            rmsd = AllChem.AlignMol(reference_mol, optimizeWithFF(test_mol))\n",
    "        except:\n",
    "            rmsd = AllChem.AlignMol(reference_mol, test_mol)\n",
    "    else:\n",
    "        rmsd = AllChem.AlignMol(reference_mol, test_mol)\n",
    "\n",
    "    return rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, debug=False, savepred_path=None, savepermol=False, useFF=False)\n",
    "\n",
    "    val_num_samples = 10 # number of conformers to draw from prior \n",
    "\n",
    "    # val batch size is different from train batch size since we use multiple samples\n",
    "    val_batch_size = int(batch_size / val_num_samples) # number of molecules to draw conformers from ; ie 2\n",
    "    n_batch_val = int(len(D1_v)/val_batch_size) # 1500 if D1_v = 3000\n",
    "    assert ((batch_size % val_num_samples) == 0)\n",
    "    assert (len(D1_v) % val_batch_size == 0)\n",
    "\n",
    "    val_size = D1_v.shape[0]\n",
    "    valscores_mean = np.zeros(val_size)\n",
    "    valscores_std = np.zeros(val_size)\n",
    "\n",
    "    if savepred_path != None:\n",
    "        if not savepermol:\n",
    "            pred_v = np.zeros(D1_v.shape[0], val_num_samples, n_max, 3)\n",
    "\n",
    "    print (\"testing model...\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            start_ = batch_idx * val_batch_size\n",
    "            end_ = start_ + val_batch_size\n",
    "\n",
    "            nodes, masks, edges, proximity, pos = batch #D1 D2 D3 D4 D5\n",
    "\n",
    "            # repeat because we want val_batch_size (molecule) * val_num_samples (conformer per molecule)\n",
    "            nodes = torch.repeat_interleave(nodes, val_num_samples, dim=0)\n",
    "            masks = torch.repeat_interleave(masks, val_num_samples, dim=0)\n",
    "            edges = torch.repeat_interleave(edges, val_num_samples, dim=0)\n",
    "            proximity = torch.repeat_interleave(proximity, val_num_samples, dim=0)\n",
    "\n",
    "            if debug:\n",
    "                print (i, len(test_loader))\n",
    "\n",
    "            _, _, _, _, _, PX_pred = model(nodes, edges, masks, pos, proximity)\n",
    "\n",
    "            if savepred_path != None:\n",
    "                if not savepermol:\n",
    "                    pred_v[start_:end_] = PX_pred.view(val_batch_size, val_num_samples, self.n_max, 3)\n",
    "\n",
    "            X_pred = PX_pred\n",
    "            for r in range(self.refine_steps):\n",
    "                if self.use_X:\n",
    "                    pos = X_pred\n",
    "                if self.use_R:\n",
    "                    proximity = pos_to_proximity(X_pred, mask)\n",
    "                _, _, _, _, last_X_pred, _ = model(nodes, edges, mask, pos, proximity)\n",
    "                X_pred = self.refine_mom * X_pred + (1-self.refine_mom) * last_X_pred\n",
    "\n",
    "            valrmsd=[]\n",
    "            for j in range(X_pred.shape[0]):\n",
    "                ms_v_index = int(j / self.val_num_samples) + start_\n",
    "                rmsd = self.getRMSD(MS_v[ms_v_index], D5_batch_pred[j], useFF)\n",
    "                valrmsd.append(rmsd)\n",
    "\n",
    "            valrmsd = np.array(valrmsd)\n",
    "            valrmsd = np.reshape(valrmsd, (val_batch_size, val_num_samples))\n",
    "            valrmsd_mean = np.mean(valrmsd, axis=1)\n",
    "            valrmsd_std = np.std(valrmsd, axis=1)\n",
    "\n",
    "            valscores_mean[start_:end_] = valrmsd_mean\n",
    "            valscores_std[start_:end_] = valrmsd_std\n",
    "\n",
    "            # save results per molecule if request\n",
    "            if savepermol:\n",
    "                pred_curr = copy.deepcopy(X_pred).view(val_batch_size, val_num_samples, n_max, 3)\n",
    "                for tt in range(0, val_batch_size):\n",
    "                    save_dict_tt = {'rmsd': valrmsd[tt], 'pred': pred_curr[tt]}\n",
    "                    pkl.dump(save_dict_tt, \\\n",
    "                        open(os.path.join(savepred_path, 'mol_{}_neuralnet.p'.format(tt+start_)), 'wb'))\n",
    "\n",
    "        print (\"val scores: mean is {} , std is {}\".format(np.mean(valscores_mean), np.mean(valscores_std)))\n",
    "        if savepred_path != None:\n",
    "            if not savepermol:\n",
    "                print (\"saving neural net predictions into {}\".format(savepred_path))\n",
    "                pkl.dump(pred_v, open(savepred_path, 'wb'))\n",
    "\n",
    "        return np.mean(valscores_mean), np.mean(valscores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--loaddir', type=str, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9ed6231d5c89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mckptdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./checkpoints/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckptdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckptdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# handled in args parse for a py script version\n",
    "\n",
    "n_max = 50\n",
    "dim_node = 35\n",
    "dim_edge = 10\n",
    "nval = 3000\n",
    "ntst = 3000\n",
    "hidden_node_dim = 50\n",
    "dim_f = 100\n",
    "batch_size = 20\n",
    "val_num_samples = 10\n",
    "model_name = 'dl4chem'\n",
    "savepermol = True\n",
    "savepreddir = 'savepreddir'\n",
    "use_val = True\n",
    "mpnn_steps = 5\n",
    "alignment_type = 'kabsch'\n",
    "tol = 1e-5\n",
    "use_X=False\n",
    "use_R=True\n",
    "seed=1334\n",
    "refine_steps=0\n",
    "refine_mom=0.99\n",
    "debug = False\n",
    "useFF = False\n",
    "w_reg = 1e-5\n",
    "log_train_steps=100\n",
    "\n",
    "\n",
    "data_dir = '/home/bb596/rds/hpc-work/dl4chem/'\n",
    "dataset = 'COD'\n",
    "COD_molset_50_path = data_dir + 'COD_molset_50.p'  \n",
    "COD_molset_all_path = data_dir + 'COD_molset_all.p' \n",
    "COD_molvec_50_path = data_dir + 'COD_molvec_50.p'\n",
    "\n",
    "# create directories to store results\n",
    "\n",
    "ckptdir = './checkpoints/'\n",
    "if not os.path.exists(args.ckptdir):\n",
    "    os.makedirs(args.ckptdir)\n",
    "    \n",
    "eventdir = './events/'\n",
    "train_eventdir = eventdir.split('/')\n",
    "train_eventdir.insert(-1, 'train')\n",
    "train_eventdir = '/'.join(train_eventdir)\n",
    "\n",
    "valid_eventdir = eventdir.split('/')\n",
    "valid_eventdir.insert(-1, 'valid')\n",
    "valid_eventdir = '/'.join(valid_eventdir)\n",
    "\n",
    "if not os.path.exists(args.train_eventdir):\n",
    "    os.makedirs(args.train_eventdir)\n",
    "if not os.path.exists(args.valid_eventdir):\n",
    "    os.makedirs(args.valid_eventdir)\n",
    "\n",
    "save_path = os.path.join(ckptdir, model_name + '_model.ckpt')\n",
    "\n",
    "molvec_fname = data_dir + dataset + '_molvec_'+str(n_max)+'.p'\n",
    "molset_fname = data_dir + dataset + '_molset_'+str(n_max)+'.p'\n",
    "\n",
    "# load data\n",
    "\n",
    "[D1, D2, D3, D4, D5] = pkl.load(open(molvec_fname,'rb'))\n",
    "D1 = D1.todense()\n",
    "D2 = D2.todense()\n",
    "D3 = D3.todense()\n",
    "\n",
    "ntrn = len(D5)-nval-ntst\n",
    "\n",
    "[molsup, molsmi] = pkl.load(open(molset_fname,'rb'))\n",
    "\n",
    "D1_trn = D1[:ntrn]\n",
    "D2_trn = D2[:ntrn]\n",
    "D3_trn = D3[:ntrn]\n",
    "D4_trn = D4[:ntrn]\n",
    "D5_trn = D5[:ntrn]\n",
    "molsup_trn =molsup[:ntrn]\n",
    "D1_val = D1[ntrn:ntrn+nval]\n",
    "D2_val = D2[ntrn:ntrn+nval]\n",
    "D3_val = D3[ntrn:ntrn+nval]\n",
    "D4_val = D4[ntrn:ntrn+nval]\n",
    "D5_val = D5[ntrn:ntrn+nval]\n",
    "molsup_val =molsup[ntrn:ntrn+nval]\n",
    "D1_tst = D1[ntrn+nval:ntrn+nval+ntst]\n",
    "D2_tst = D2[ntrn+nval:ntrn+nval+ntst]\n",
    "D3_tst = D3[ntrn+nval:ntrn+nval+ntst]\n",
    "D4_tst = D4[ntrn+nval:ntrn+nval+ntst]\n",
    "D5_tst = D5[ntrn+nval:ntrn+nval+ntst]\n",
    "molsup_tst =molsup[ntrn+nval:ntrn+nval+ntst]\n",
    "print ('::: num train samples is ')\n",
    "print(D1_trn.shape, D3_trn.shape)\n",
    "\n",
    "tm_trn, tm_val, tm_tst = None, None, None\n",
    "\n",
    "del D1, D2, D3, D4, D5, molsup\n",
    "\n",
    "if savepermol:\n",
    "    savepreddir = os.path.join(savepreddir, dataset, \"_val_\" if use_val else \"_test_\")\n",
    "    if not os.path.exists(args.savepreddir):\n",
    "        os.makedirs(args.savepreddir)\n",
    "        \n",
    "model = CoordAE(n_max, dim_node, dim_edge, hidden_node_dim, dim_f, batch_size, val_num_samples, \\\n",
    "                    mpnn_steps=mpnn_steps, alignment_type=alignment_type, tol=tol,\\\n",
    "                    use_X=use_X, use_R=use_R, seed=seed, \\\n",
    "                    refine_steps=refine_steps, refine_mom=refine_mom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(D1_trn, D2_trn, D3_trn, D4_trn, D5_trn, molsup_trn, \\\n",
    "                D1_val, D2_val, D3_val, D4_val, D5_val, molsup_val, \\\n",
    "                load_path=args.loaddir, save_path=save_path, \\\n",
    "                train_event_path=args.train_eventdir, valid_event_path=args.valid_eventdir, \\\n",
    "                log_train_steps=args.log_train_steps, tm_trn=tm_trn, tm_val=tm_val, \\\n",
    "                w_reg=args.w_reg, \\\n",
    "                debug=args.debug, exp=exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(D1_val, D2_val, D3_val, D4_val, D5_val, molsup_val, \\\n",
    "                    load_path=args.loaddir, tm_v=tm_val, debug=args.debug, \\\n",
    "                    savepred_path=args.savepreddir, savepermol=args.savepermol, useFF=args.useFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(D1_tst, D2_tst, D3_tst, D4_tst, D5_tst, molsup_tst, \\\n",
    "                    load_path=args.loaddir, tm_v=tm_tst, debug=args.debug, \\\n",
    "                    savepred_path=args.savepreddir, savepermol=args.savepermol, useFF=args.useFF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
